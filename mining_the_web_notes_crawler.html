<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html>
   
<!-- Mirrored from www.keithmcdonnell.net/mining_the_web_notes_crawler.html by HTTrack Website Copier/3.x [XR&CO'2013], Thu, 03 Apr 2014 18:49:46 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
      <meta http-equiv="Content-type" content="text/html; charset=utf-8" />
      <link href="dancingtext.css" rel="stylesheet" type="text/css" media="screen"/>
      <title>Mining the web reading notes crawler</title>
   </head>
   <body class='show'>
      <h1>Mining the web reading notes crawler</h1>

<p class='author'>Written by <a href="index.html">Keith McDonnell</a>. Last updated on 
Saturday, March 03, 2012.</p>

<div class='content'>                                 
   <p>= Questions</p>
<p>Is erlang a good choice for web crawling?<br />
- yes for a scalable server<br />
- might be too much to learn a new language &amp; IR concepts</p>
<p>What is a good starter project?<br />
- benchmark a crawler in ruby</p>
<p>Should I build a scalable crawler or proof of concept?<br />
- scalable proof of concept (all sections displayed in fig 2.2 pp 21)</p>
<p>Are there ruby libraries for:<br />
- non blocking IO<br />
- <span class="caps">DNS</span> fetching<br />
- Canonical <span class="caps">URLS</span><br />
- wrapper for libwww  C library</p>
<p>Should I build a basic crawler first (see Mercurio Fernandez&#8217;s eigenclass crawler,<br />
or check github).</p>
<p>= Read</p>
<p>Simple crawler</p>
<p>The central function of a crawler is to fetch many pages at the same time, in<br />
order to overlap the delays involved in<br />
1. Resolving the hostname in the <span class="caps">URL</span> to an IP address using <span class="caps">DNS</span><br />
2. Connecting a socket to the server and sending the request<br />
3. Receiving the requested page in response</p>
<p>Find anatomy of a large-scale crawler diagram</p>
<p>Because a single page fetch may involve several seconds of network latency, it<br />
is essential to fetch many pages (typically hundreds to thousands) at the same<br />
time to utilize the network bandwidth available.</p>
<p>Many simultaneous fetches are possible only if the <span class="caps">DNS</span> lookup is streamlined<br />
to be highly concurrent, possibly replicated on a few <span class="caps">DNS</span> servers.</p>
<p>Multiprocessing or multithreading provided by the operating system is not the<br />
best way to manage the multiple fetches owing to high overheads. The best<br />
bet is to explicitly encode the state of a fetch context in a data structure and<br />
use asynchronous sockets, which do not block the process/thread using it, but<br />
can be polled to check for completion of network transfers.</p>
<p>Care is needed to extract URLs and eliminate duplicates to reduce redundant<br />
fetches and to avoid “spider traps”—hyperlink graphs constructed carelessly<br />
or malevolently to keep a crawler trapped in that graph, fetching what can<br />
potentially be an infinite set of “fake” URLs.</p>
<p>a canonical <span class="caps">URL</span> is formed by the following steps:<br />
1. A standard string is used for the protocol (most browsers tolerate Http, which<br />
should be converted to lowercase, for example).<br />
2. The hostname is canonicalized as mentioned above.<br />
3. An explicit port number is added if necessary.<br />
4. The path is normalized and cleaned up, for example, /books/../papers<br />
/sigmod1999.ps simplifies to /papers/sigmod1999.ps.</p>
<p>It is easiest to start building a crawler with a core whose only responsibility is to<br />
copy bytes from network sockets to storage media: this is the Crawler class. The<br />
Crawler’s contract with the user is expressed in these methods:</p>
<p>class Crawler {</p>
<ul>
	<li>void setDnsTimeout(int milliSeconds);</li>
	<li>void setHttpTimeout(int milliSeconds);</li>
	<li>void fetchPush(const string&amp; url);</li>
	<li>virtual boolean fetchPull(string&amp; url); // set url, return success</li>
	<li>virtual void fetchDone(const string&amp; url,</li>
	<li>const ReturnCode returnCode, // timeout, server not found, &#8230;</li>
	<li>const int httpStatus,</li>
	<li>// 200, 404, 302, &#8230;</li>
	<li>const hash_map&lt;string, string&gt;&amp; mimeHeader,</li>
	<li>// Content-type = text/html</li>
	<li>// Last-modified = &#8230;</li>
	<li>const unsigned char * pageBody,</li>
	<li>const size_t pageSize);<br />
};</li>
</ul>
<p>Implementation of the Crawler class. <br />
We will need two helper classes called <span class="caps">DNS</span> and Fetch. Crawler is started with a fixed set of <span class="caps">DNS</span> servers.<br />
For each server, a <span class="caps">DNS</span> object is created.</p>
<p>Each <span class="caps">DNS</span> object creates a <span class="caps">UDP</span> socket with its assigned <span class="caps">DNS</span> server as the destination. <br />
The most important data structure included in a <span class="caps">DNS</span> object is a list of Fetch <br />
contexts waiting for the corresponding <span class="caps">DNS</span> server to respond</p>
<p>class <span class="caps">DNS</span> {</p>
<ul>
	<li>list&lt;Fetch*&gt; waitForDns;<br />
}</li>
</ul>
<p>Crawler::start()</p>
<p>while event loop has not been stopped do</p>
<ul>
	<li>if not enough active Fetches to keep system busy then</li>
	<li>try a fetchPull to replenish the system with more work</li>
	<li>if no pending Fetches in the system then</li>
	<li>stop the event loop</li>
	<li>end if</li>
	<li>end if</li>
	<li></li>
	<li>if not enough Http sockets busy and there is a Fetch f in waitForHttp whose server IP address busyServers then</li>
	<li>remove f from waitForHttp</li>
	<li>lock the IP address by adding an entry to busyServers (to be polite to the server)</li>
	<li>change f.state to STATE_HTTP_SEND</li>
	<li>allocate an Http socket s to start the Http protocol</li>
	<li>set the Http timeout for f</li>
	<li>set httpSockets[s] to the Fetch pointer</li>
	<li>continue the outermost event loop</li>
	<li>end if</li>
	<li></li>
	<li>if the shortest waitForDns is “too short” then</li>
	<li>remove a <span class="caps">URL</span> from the head of waitForIssue</li>
	<li>create a Fetch object f with this <span class="caps">URL</span></li>
	<li>issue the <span class="caps">DNS</span> request for f</li>
	<li>set f.state to STATE_DNS_RECEIVE</li>
	<li>set the <span class="caps">DNS</span> timeout for f</li>
	<li>put f on the laziest DNS’s waitForDns</li>
	<li>continue the outermost event loop</li>
	<li>end if</li>
	<li></li>
	<li>collect open SocketIDs from dnsSockets and httpSockets</li>
	<li>also collect the earliest deadline over all active Fetches</li>
	<li>perform the select call on the open sockets with the earliest deadline</li>
	<li>as timeout</li>
	<li></li>
	<li>if select returned with a timeout then</li>
	<li>for each expired Fetch record f in dnsSockets and httpSockets do</li>
	<li>remove f</li>
	<li>invoke f.fetchDone(&#8230;) with suitable timeout error codes</li>
	<li>remove any locks held in busyServers</li>
	<li>end for</li>
	<li>else</li>
	<li>find a SocketID s that is ready for read or write</li>
	<li>locate a Fetch record f in dnsSockets or httpSockets that was waiting on s</li>
	<li></li>
	<li>if a <span class="caps">DNS</span> request has been completed for f then</li>
	<li>move f from waitForDns to waitForHttp</li>
	<li>else</li>
	<li>if socket is ready for writing then</li>
	<li>send request</li>
	<li>change f.state to STATE_HTTP_RECEIVE</li>
	<li>else</li>
	<li>receive more bytes</li>
	<li>if receive completed then</li>
	<li>invoke f.fetchDone(&#8230;) with successful status codes</li>
	<li>remove any locks held in busyServers</li>
	<li>remove f from waitForHttp and destroy f</li>
	<li>end if</li>
	<li>end if</li>
	<li>end if</li>
	<li>end if<br />
end while</li>
</ul>
</div>

<p class='comments'>
   If you&apos;d like to discuss this article, you can send me an email 
   <a href="mailto:&#107;&#101;&#105;&#116;&#104;&#64;&#100;&#97;&#110;&#99;&#105;&#110;&#103;&#116;&#101;&#120;&#116;&#46;&#99;&#111;&#109;">&#107;&#101;&#105;&#116;&#104;&#64;&#100;&#97;&#110;&#99;&#105;&#110;&#103;&#116;&#101;&#120;&#116;&#46;&#99;&#111;&#109;</a> and/or 
   publish an article online and link back to this page.
</p>

      
      <script type="text/javascript">
      var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
      document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
      </script>
      <script type="text/javascript">
      try {
      var pageTracker = _gat._getTracker("UA-8421111-1");
      pageTracker._trackPageview();
      } catch(err) {}</script>
      
      
      <p class='copyright'>
         <a href="http://creativecommons.org/licenses/by-sa/2.5/">Copyright &copy;</a>
         2006 - 2014, 
         <a href="index.html">Keith McDonnell</a>.
      </p>
      
   </body>

<!-- Mirrored from www.keithmcdonnell.net/mining_the_web_notes_crawler.html by HTTrack Website Copier/3.x [XR&CO'2013], Thu, 03 Apr 2014 18:49:46 GMT -->
</html>